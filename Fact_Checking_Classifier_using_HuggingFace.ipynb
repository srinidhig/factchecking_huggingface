{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8a3fbd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5f51de5",
    "outputId": "e467fe09-f29d-4df9-949f-2f1552b21e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==3.0.0 in c:\\users\\srini\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\srini\\anaconda3\\lib\\site-packages (from transformers==3.0.0) (3.0.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\srini\\anaconda3\\lib\\site-packages (from transformers==3.0.0) (20.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\srini\\anaconda3\\lib\\site-packages (from transformers==3.0.0) (1.20.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\srini\\anaconda3\\lib\\site-packages (from transformers==3.0.0) (0.1.95)\n",
      "Requirement already satisfied: tokenizers==0.8.0-rc4 in c:\\users\\srini\\anaconda3\\lib\\site-packages (from transformers==3.0.0) (0.8.0rc4)\n",
      "Requirement already satisfied: requests in c:\\users\\srini\\appdata\\roaming\\python\\python37\\site-packages (from transformers==3.0.0) (2.23.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\srini\\anaconda3\\lib\\site-packages (from transformers==3.0.0) (0.0.45)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\srini\\anaconda3\\lib\\site-packages (from transformers==3.0.0) (4.59.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\srini\\anaconda3\\lib\\site-packages (from transformers==3.0.0) (2021.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\srini\\anaconda3\\lib\\site-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srini\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers==3.0.0) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\srini\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\srini\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers==3.0.0) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\srini\\appdata\\roaming\\python\\python37\\site-packages (from requests->transformers==3.0.0) (1.25.8)\n",
      "Requirement already satisfied: click in c:\\users\\srini\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\srini\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\srini\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.0) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Install the required packages\n",
    "\n",
    "!pip install transformers==3.0.0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import torch.nn.functional as F\n",
    "from transformers import pipeline, BertTokenizer, BertModel, TFBertModel, AutoModel, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "63ccf436",
   "metadata": {
    "id": "b8952e13"
   },
   "outputs": [],
   "source": [
    "# Import train, dev, and test data\n",
    "\n",
    "train = pd.read_csv('train.tsv', sep = '\\t')\n",
    "dev = pd.read_csv('dev.tsv', sep = '\\t')\n",
    "test = pd.read_csv('test.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "629ccc79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "802b0a0e",
    "outputId": "eb1cb34f-9f1d-44c0-b449-a01f21d5659b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>claim</th>\n",
       "      <th>date_published</th>\n",
       "      <th>explanation</th>\n",
       "      <th>fact_checkers</th>\n",
       "      <th>main_text</th>\n",
       "      <th>sources</th>\n",
       "      <th>label</th>\n",
       "      <th>subjects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15661</td>\n",
       "      <td>\"The money the Clinton Foundation took from fr...</td>\n",
       "      <td>April 26, 2015</td>\n",
       "      <td>\"Gingrich said the Clinton Foundation \"\"took m...</td>\n",
       "      <td>Katie Sanders</td>\n",
       "      <td>\"Hillary Clinton is in the political crosshair...</td>\n",
       "      <td>https://www.wsj.com/articles/clinton-foundatio...</td>\n",
       "      <td>false</td>\n",
       "      <td>Foreign Policy, PunditFact, Newt Gingrich,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9893</td>\n",
       "      <td>Annual Mammograms May Have More False-Positives</td>\n",
       "      <td>October 18, 2011</td>\n",
       "      <td>This article reports on the results of a study...</td>\n",
       "      <td></td>\n",
       "      <td>While the financial costs of screening mammogr...</td>\n",
       "      <td></td>\n",
       "      <td>mixture</td>\n",
       "      <td>Screening,WebMD,women's health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11358</td>\n",
       "      <td>SBRT Offers Prostate Cancer Patients High Canc...</td>\n",
       "      <td>September 28, 2016</td>\n",
       "      <td>This news release describes five-year outcomes...</td>\n",
       "      <td>Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...</td>\n",
       "      <td>The news release quotes lead researcher Robert...</td>\n",
       "      <td>https://www.healthnewsreview.org/wp-content/up...</td>\n",
       "      <td>mixture</td>\n",
       "      <td>Association/Society news release,Cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10166</td>\n",
       "      <td>Study: Vaccine for Breast, Ovarian Cancer Has ...</td>\n",
       "      <td>November 8, 2011</td>\n",
       "      <td>While the story does many things well, the ove...</td>\n",
       "      <td></td>\n",
       "      <td>The story does discuss costs, but the framing ...</td>\n",
       "      <td>http://clinicaltrials.gov/ct2/results?term=can...</td>\n",
       "      <td>true</td>\n",
       "      <td>Cancer,WebMD,women's health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11276</td>\n",
       "      <td>Some appendicitis cases may not require ’emerg...</td>\n",
       "      <td>September 20, 2010</td>\n",
       "      <td>We really don’t understand why only a handful ...</td>\n",
       "      <td></td>\n",
       "      <td>\"Although the story didn’t cite the cost of ap...</td>\n",
       "      <td></td>\n",
       "      <td>true</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  claim_id                                              claim  \\\n",
       "0    15661  \"The money the Clinton Foundation took from fr...   \n",
       "1     9893    Annual Mammograms May Have More False-Positives   \n",
       "2    11358  SBRT Offers Prostate Cancer Patients High Canc...   \n",
       "3    10166  Study: Vaccine for Breast, Ovarian Cancer Has ...   \n",
       "4    11276  Some appendicitis cases may not require ’emerg...   \n",
       "\n",
       "       date_published                                        explanation  \\\n",
       "0      April 26, 2015  \"Gingrich said the Clinton Foundation \"\"took m...   \n",
       "1    October 18, 2011  This article reports on the results of a study...   \n",
       "2  September 28, 2016  This news release describes five-year outcomes...   \n",
       "3    November 8, 2011  While the story does many things well, the ove...   \n",
       "4  September 20, 2010  We really don’t understand why only a handful ...   \n",
       "\n",
       "                                       fact_checkers  \\\n",
       "0                                      Katie Sanders   \n",
       "1                                                      \n",
       "2  Mary Chris Jaklevic,Steven J. Atlas, MD, MPH,K...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                                           main_text  \\\n",
       "0  \"Hillary Clinton is in the political crosshair...   \n",
       "1  While the financial costs of screening mammogr...   \n",
       "2  The news release quotes lead researcher Robert...   \n",
       "3  The story does discuss costs, but the framing ...   \n",
       "4  \"Although the story didn’t cite the cost of ap...   \n",
       "\n",
       "                                             sources    label  \\\n",
       "0  https://www.wsj.com/articles/clinton-foundatio...    false   \n",
       "1                                                     mixture   \n",
       "2  https://www.healthnewsreview.org/wp-content/up...  mixture   \n",
       "3  http://clinicaltrials.gov/ct2/results?term=can...     true   \n",
       "4                                                        true   \n",
       "\n",
       "                                      subjects  \n",
       "0  Foreign Policy, PunditFact, Newt Gingrich,   \n",
       "1               Screening,WebMD,women's health  \n",
       "2      Association/Society news release,Cancer  \n",
       "3                  Cancer,WebMD,women's health  \n",
       "4                                               "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "65285bfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "927592bc",
    "outputId": "e9888c28-72ad-4470-e9a6-5366b884a7d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Annual Mammograms May Have More False-Positives'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze what the text looks like\n",
    "\n",
    "train['claim'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c523410e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "1e6ecba4",
    "outputId": "d817f1fc-1f42-4b22-a4cb-3e6468996fe0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This article reports on the results of a study of nearly 170,000 women who had screening mammograms beginning between age 40-59. The study found that over ten years of screening mammograms, over half of the women will experience a false-positive recall for additional mammography. In addition, 7%-9% of the women will have a biopsy for a suspicious lump which is not cancerous. Both of those percentages decrease if the woman is screened every other year rather than every year. Even with biennial mammography, 41% of women will experience a\\xa0recall over 10 years of mammography. The study’s Principal Investigator emphasized that “in most cases, a recall doesn’t mean you have cancer.”\\xa0 She hoped this knowledge would reduce the anxiety of women who are recalled. The story never explained the size of the decrease in the number of false positives between annual (61.3%) and biennial screening (41.6%). Our first two reviewers were a researcher who specializes in health decisions and a breast cancer survivor trained in evidence by the Natiional Breast Cancer Coalition’s Project LEAD. This study is valuable because it helps to quantify and compare the harms of annual and biennial screening, specifically the number of false positives and the number of unnecessary biopsies. Prior to this study, estimates of false positive screening mammography rates varied widely. The critical question is whether you can do less frequent screening, subject women to fewer harms and get similar results in terms of detection of “early stage” cancer. This study’s data seems to suggest that answer is yes.'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['explanation'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "68e391a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "fad89ca4",
    "outputId": "848fabe4-02fb-44d0-bae5-1919f4a76792"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While the financial costs of screening mammography & false-positive recalls & biopsies were not included in the study, readers would benefit from being reminded that recall mammography & biopsies increase patient financial costs. This article leads with valuable information for all women over age 40 by stating “Getting screening mammograms every two years instead of annually reduces the chance of a false alarm, a new study shows.”\\xa0 Unfortunately the writer doesn’t quantify or elaborate adequately on that reduction. Instead, the writer later focuses on how women undergoing screening mammography shouldn’t be anxious if they experience a recall because over half of women who have screening mammograms for 10 years will be recalled. Readers would have been better served if the writer had emphasized the significant reduction in both recall mammograms & false-positive biopsies in women who are screened every two years rather than annually. Part of the continuing controversy\\xa0over screening mammography\\xa0focuses on\\xa0annual versus biennial screening. Because this\\xa0study showed a significant reduction in the percentage of those recalled & needing biopsies among women who are screened\\xa0every other year, with no\\xa0statistically significant increase in late-stage diagnosis of breast cancer, the article should\\xa0have emphasized those important findings. The piece states that the researchers noted that “…false positive recalls may cause inconvenience & anxiety and biopsies can cause pain and scarring.” This article fails to include several important facts about the study. 1) This is a prospective cohort study of women screened between 1994-2006. 2) Most of the mammograms were film rather than digital. 3) Few women underwent screening for the entire 10 year period. 4) Screening mammography recall rates are influenced by the skill of the radiologists who read the mammograms. There was no disease mongering. No independent experts were quoted. The story did an adequate job comparing – at a very high level – annual screening versus biennial screening. Screening mammography is widely available throughout the United States. The study did not examine a new procedure. There’s no evidence that the story relied solely on a news release.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['main_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aabc600c",
   "metadata": {
    "id": "0e178024"
   },
   "outputs": [],
   "source": [
    "# Take only the 4 important labels for classification\n",
    "# Train and dev had some additional labels with a very lo frequency so those are removed.\n",
    "\n",
    "labels = ['true', 'false', 'mixture', 'unproven']\n",
    "train = train[train.label.isin(labels)]\n",
    "dev = dev[dev.label.isin(labels)]\n",
    "test = test[test.label.isin(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "00abe4b8",
   "metadata": {
    "id": "79c27ded"
   },
   "outputs": [],
   "source": [
    "# Creatig a cumulative dataframe that contains all the text\n",
    "\n",
    "cols = ['claim_id', 'claim', 'date_published', 'explanation', 'fact_checkers',\n",
    "       'main_text', 'sources', 'label', 'subjects']\n",
    "\n",
    "df = train[cols].append(dev[cols]).append(test[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1bbef64f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a5cbd43",
    "outputId": "9d662b38-9030-4dc9-d657-04af66f8fb2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "claim_id             0\n",
       "claim                0\n",
       "date_published    2398\n",
       "explanation          0\n",
       "fact_checkers        0\n",
       "main_text            0\n",
       "sources              1\n",
       "label                0\n",
       "subjects             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check nmber of missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a25075e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true        6306\n",
       "false       3769\n",
       "mixture     1799\n",
       "unproven     377\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of labels\n",
    "\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ec90d345",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "30729890",
    "outputId": "ae26fc0a-a410-4540-fb86-9153d9e170ef",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get lengths of claims, explanation, and main text to see how much to pad them\n",
    "seq_len_maintext = [len(i.split()) for i in df.main_text]\n",
    "seq_len_claim = [len(i.split()) for i in df.claim]\n",
    "seq_len_explanation = [len(i.split()) for i in df.explanation]\n",
    "\n",
    "pd.Series(seq_len_maintext).hist(bins = 30)\n",
    "plt.title('Distribution of main_text length')\n",
    "plt.show()\n",
    "pd.Series(seq_len_explanation).hist(bins = 30)\n",
    "plt.title('Distribution of explanation length')\n",
    "plt.show()\n",
    "pd.Series(seq_len_claim).hist(bins = 30)\n",
    "plt.title('Distribution of claim length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2bc904fb",
   "metadata": {
    "id": "91fa9691"
   },
   "outputs": [],
   "source": [
    "# Create a new column, appending claim, explanation and main text\n",
    "\n",
    "train['input_text'] = train.claim + ' ' + train.explanation + ' ' + train.main_text\n",
    "dev['input_text'] = dev.claim + ' ' + dev.explanation + ' ' + dev.main_text\n",
    "test['input_text'] = test.claim + ' ' + test.explanation + ' ' + test.main_text\n",
    "df['input_text'] = df.claim + ' ' + df.explanation + ' ' + df.main_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "99130146",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "fe192f42",
    "outputId": "671e1486-f8a0-4a47-b2d8-b75c20f83024"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len_input_text = [len(i.split()) for i in df.input_text]\n",
    "\n",
    "pd.Series(seq_len_input_text).hist(bins = 30)\n",
    "plt.title('Distribution of total input text length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1030136a",
   "metadata": {
    "id": "1789a947"
   },
   "outputs": [],
   "source": [
    "# We can fix a max length of 256 and if less than that, add padding. Not setting to higher due to model and training time\n",
    "# constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48c9caa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7d52cc7",
    "outputId": "21669745-c6ac-4e8d-f18e-0a858b67a179",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srini\\anaconda3\\lib\\site-packages\\transformers\\modeling_auto.py:791: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09019200503826141,\n",
       "  'sequence': \"[CLS] Hello I'm a fashion model. [SEP]\",\n",
       "  'token': 4633,\n",
       "  'token_str': 'fashion'},\n",
       " {'score': 0.063500314950943,\n",
       "  'sequence': \"[CLS] Hello I'm a new model. [SEP]\",\n",
       "  'token': 1207,\n",
       "  'token_str': 'new'},\n",
       " {'score': 0.06228220462799072,\n",
       "  'sequence': \"[CLS] Hello I'm a male model. [SEP]\",\n",
       "  'token': 2581,\n",
       "  'token_str': 'male'},\n",
       " {'score': 0.04417290911078453,\n",
       "  'sequence': \"[CLS] Hello I'm a professional model. [SEP]\",\n",
       "  'token': 1848,\n",
       "  'token_str': 'professional'},\n",
       " {'score': 0.033261489123106,\n",
       "  'sequence': \"[CLS] Hello I'm a super model. [SEP]\",\n",
       "  'token': 7688,\n",
       "  'token_str': 'super'}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
    "unmasker(\"Hello I'm a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9847e9c",
   "metadata": {
    "id": "75b93051",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model to use\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "32d98493",
   "metadata": {
    "id": "2XP_Mu4X__3l"
   },
   "outputs": [],
   "source": [
    "# Take only percentage of train/dev data - if required. To speed up training and evaluating\n",
    "train = train[:8192]\n",
    "dev = dev[:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f4a5dfeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92ee2476",
    "outputId": "7783a8d2-83ad-4492-bce9-87cde817477c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train.input_text.tolist(),\n",
    "    max_length = 256,\n",
    "    padding = 'max_length',\n",
    "    truncation=True)\n",
    "print('Done')\n",
    "\n",
    "# tokenize and encode sequences in the dev set\n",
    "tokens_dev = tokenizer.batch_encode_plus(\n",
    "    dev.input_text.tolist(),\n",
    "    max_length = 256,\n",
    "    padding = 'max_length',\n",
    "    truncation=True)\n",
    "print('Done')\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test.input_text.tolist(),\n",
    "    max_length = 256,\n",
    "    padding = 'max_length',\n",
    "    truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "be6c0ade",
   "metadata": {
    "id": "ad9a8eb5"
   },
   "outputs": [],
   "source": [
    "# Encode labels to numeric values 0,1,2,3\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_labels = le.fit_transform(train.label.tolist())\n",
    "dev_labels = le.transform(dev.label.tolist())\n",
    "test_labels = le.transform(test.label.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9e74e8f1",
   "metadata": {
    "id": "2f2c4514"
   },
   "outputs": [],
   "source": [
    "## convert lists to torch tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "#train_y = nn.functional.one_hot(torch.Tensor(train_labels).to(torch.int64), num_classes = 4)\n",
    "train_y = torch.Tensor(train_labels).to(torch.int64)\n",
    "\n",
    "dev_seq = torch.tensor(tokens_dev['input_ids'])\n",
    "dev_mask = torch.tensor(tokens_dev['attention_mask'])\n",
    "#dev_y = nn.functional.one_hot(torch.Tensor(dev_labels).to(torch.int64), num_classes = 4)\n",
    "dev_y = torch.Tensor(dev_labels).to(torch.int64)\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "#test_y = nn.functional.one_hot(torch.Tensor(test_labels).to(torch.int64), num_classes = 4)\n",
    "test_y = torch.Tensor(test_labels).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "41197814",
   "metadata": {
    "id": "8012d871"
   },
   "outputs": [],
   "source": [
    "# Split train, test, and dev data into batches\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batch_size)\n",
    "\n",
    "dev_data = TensorDataset(dev_seq, dev_mask, dev_y)\n",
    "dev_sampler = SequentialSampler(dev_data)\n",
    "dev_dataloader = DataLoader(dev_data, sampler = dev_sampler, batch_size = batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d8eab8ba",
   "metadata": {
    "id": "6a604e3a"
   },
   "outputs": [],
   "source": [
    "# Fix bert model architecture - no fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7633aa7f",
   "metadata": {
    "id": "0b6c9f24"
   },
   "outputs": [],
   "source": [
    "# Add to the existing BERT architecture the final layers that perform the multiclass classification task\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert\n",
    "        # Add dropout\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        # ReLU activation\n",
    "        self.relu =  nn.ReLU()\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        # Final layer\n",
    "        self.fc2 = nn.Linear(512,4)\n",
    "        \n",
    "        # Softmax for multiclass likelihoods\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask = mask)\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c6c13255",
   "metadata": {
    "id": "d69ff279"
   },
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our defined architecture\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "transfer_model = BERT_Arch(model)\n",
    "#transfer_model = transfer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "79bc8709",
   "metadata": {
    "id": "ad7214df"
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = AdamW(transfer_model.parameters(), lr = 1e-5)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d4295ef3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ab5f1a0",
    "outputId": "695f5364-9098-4199-9171-51be13dfe68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.81691264 1.68698517 0.48438978 8.42798354]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srini\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass classes=[0 1 2 3], y=[0 1 1 ... 1 1 0] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Due to class imbalance among claim classes, introduce class weights based on proportion in data\n",
    "class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1a5d781a",
   "metadata": {
    "id": "AUPCI7XrNIOZ"
   },
   "outputs": [],
   "source": [
    "# Define accuracy per class and f1-score (Not used)\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    #label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "00394715",
   "metadata": {
    "id": "8c433df8"
   },
   "outputs": [],
   "source": [
    "# Convert list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "cross_entropy  = nn.CrossEntropyLoss(weight = weights) \n",
    "\n",
    "# Define number of training epochs\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5180cb66",
   "metadata": {
    "id": "Or_dQTvwt4HA"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "    transfer_model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        print(step)\n",
    "        # progress update after every 10 batches.\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        transfer_model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = transfer_model(sent_id, mask)\n",
    "        print('train done')\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)\n",
    "        print(step)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes)\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50ebd9cb",
   "metadata": {
    "id": "I16xRasdt4Tl"
   },
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate(dev_dataloader):\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    transfer_model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(dev_dataloader):\n",
    "        print(step)\n",
    "        # Progress update every 10 batches.\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dev_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd so that gradients don't update during evaluation\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # model predictions\n",
    "            preds = transfer_model(sent_id, mask)\n",
    "            print('eval done')\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(dev_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ee260a6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iec9DfILcgkJ",
    "outputId": "1b3d8aa9-41af-43a7-840f-df0883e6582b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "0\n",
      "train done\n",
      "0\n",
      "1\n",
      "train done\n",
      "1\n",
      "2\n",
      "train done\n",
      "2\n",
      "3\n",
      "train done\n",
      "3\n",
      "4\n",
      "train done\n",
      "4\n",
      "5\n",
      "train done\n",
      "5\n",
      "6\n",
      "train done\n",
      "6\n",
      "7\n",
      "train done\n",
      "7\n",
      "8\n",
      "train done\n",
      "8\n",
      "9\n",
      "train done\n",
      "9\n",
      "10\n",
      "  Batch    10  of    256.\n",
      "train done\n",
      "10\n",
      "11\n",
      "train done\n",
      "11\n",
      "12\n",
      "train done\n",
      "12\n",
      "13\n",
      "train done\n",
      "13\n",
      "14\n",
      "train done\n",
      "14\n",
      "15\n",
      "train done\n",
      "15\n",
      "16\n",
      "train done\n",
      "16\n",
      "17\n",
      "train done\n",
      "17\n",
      "18\n",
      "train done\n",
      "18\n",
      "19\n",
      "train done\n",
      "19\n",
      "20\n",
      "  Batch    20  of    256.\n",
      "train done\n",
      "20\n",
      "21\n",
      "train done\n",
      "21\n",
      "22\n",
      "train done\n",
      "22\n",
      "23\n",
      "train done\n",
      "23\n",
      "24\n",
      "train done\n",
      "24\n",
      "25\n",
      "train done\n",
      "25\n",
      "26\n",
      "train done\n",
      "26\n",
      "27\n",
      "train done\n",
      "27\n",
      "28\n",
      "train done\n",
      "28\n",
      "29\n",
      "train done\n",
      "29\n",
      "30\n",
      "  Batch    30  of    256.\n",
      "train done\n",
      "30\n",
      "31\n",
      "train done\n",
      "31\n",
      "32\n",
      "train done\n",
      "32\n",
      "33\n",
      "train done\n",
      "33\n",
      "34\n",
      "train done\n",
      "34\n",
      "35\n",
      "train done\n",
      "35\n",
      "36\n",
      "train done\n",
      "36\n",
      "37\n",
      "train done\n",
      "37\n",
      "38\n",
      "train done\n",
      "38\n",
      "39\n",
      "train done\n",
      "39\n",
      "40\n",
      "  Batch    40  of    256.\n",
      "train done\n",
      "40\n",
      "41\n",
      "train done\n",
      "41\n",
      "42\n",
      "train done\n",
      "42\n",
      "43\n",
      "train done\n",
      "43\n",
      "44\n",
      "train done\n",
      "44\n",
      "45\n",
      "train done\n",
      "45\n",
      "46\n",
      "train done\n",
      "46\n",
      "47\n",
      "train done\n",
      "47\n",
      "48\n",
      "train done\n",
      "48\n",
      "49\n",
      "train done\n",
      "49\n",
      "50\n",
      "  Batch    50  of    256.\n",
      "train done\n",
      "50\n",
      "51\n",
      "train done\n",
      "51\n",
      "52\n",
      "train done\n",
      "52\n",
      "53\n",
      "train done\n",
      "53\n",
      "54\n",
      "train done\n",
      "54\n",
      "55\n",
      "train done\n",
      "55\n",
      "56\n",
      "train done\n",
      "56\n",
      "57\n",
      "train done\n",
      "57\n",
      "58\n",
      "train done\n",
      "58\n",
      "59\n",
      "train done\n",
      "59\n",
      "60\n",
      "  Batch    60  of    256.\n",
      "train done\n",
      "60\n",
      "61\n",
      "train done\n",
      "61\n",
      "62\n",
      "train done\n",
      "62\n",
      "63\n",
      "train done\n",
      "63\n",
      "64\n",
      "train done\n",
      "64\n",
      "65\n",
      "train done\n",
      "65\n",
      "66\n",
      "train done\n",
      "66\n",
      "67\n",
      "train done\n",
      "67\n",
      "68\n",
      "train done\n",
      "68\n",
      "69\n",
      "train done\n",
      "69\n",
      "70\n",
      "  Batch    70  of    256.\n",
      "train done\n",
      "70\n",
      "71\n",
      "train done\n",
      "71\n",
      "72\n",
      "train done\n",
      "72\n",
      "73\n",
      "train done\n",
      "73\n",
      "74\n",
      "train done\n",
      "74\n",
      "75\n",
      "train done\n",
      "75\n",
      "76\n",
      "train done\n",
      "76\n",
      "77\n",
      "train done\n",
      "77\n",
      "78\n",
      "train done\n",
      "78\n",
      "79\n",
      "train done\n",
      "79\n",
      "80\n",
      "  Batch    80  of    256.\n",
      "train done\n",
      "80\n",
      "81\n",
      "train done\n",
      "81\n",
      "82\n",
      "train done\n",
      "82\n",
      "83\n",
      "train done\n",
      "83\n",
      "84\n",
      "train done\n",
      "84\n",
      "85\n",
      "train done\n",
      "85\n",
      "86\n",
      "train done\n",
      "86\n",
      "87\n",
      "train done\n",
      "87\n",
      "88\n",
      "train done\n",
      "88\n",
      "89\n",
      "train done\n",
      "89\n",
      "90\n",
      "  Batch    90  of    256.\n",
      "train done\n",
      "90\n",
      "91\n",
      "train done\n",
      "91\n",
      "92\n",
      "train done\n",
      "92\n",
      "93\n",
      "train done\n",
      "93\n",
      "94\n",
      "train done\n",
      "94\n",
      "95\n",
      "train done\n",
      "95\n",
      "96\n",
      "train done\n",
      "96\n",
      "97\n",
      "train done\n",
      "97\n",
      "98\n",
      "train done\n",
      "98\n",
      "99\n",
      "train done\n",
      "99\n",
      "100\n",
      "  Batch   100  of    256.\n",
      "train done\n",
      "100\n",
      "101\n",
      "train done\n",
      "101\n",
      "102\n",
      "train done\n",
      "102\n",
      "103\n",
      "train done\n",
      "103\n",
      "104\n",
      "train done\n",
      "104\n",
      "105\n",
      "train done\n",
      "105\n",
      "106\n",
      "train done\n",
      "106\n",
      "107\n",
      "train done\n",
      "107\n",
      "108\n",
      "train done\n",
      "108\n",
      "109\n",
      "train done\n",
      "109\n",
      "110\n",
      "  Batch   110  of    256.\n",
      "train done\n",
      "110\n",
      "111\n",
      "train done\n",
      "111\n",
      "112\n",
      "train done\n",
      "112\n",
      "113\n",
      "train done\n",
      "113\n",
      "114\n",
      "train done\n",
      "114\n",
      "115\n",
      "train done\n",
      "115\n",
      "116\n",
      "train done\n",
      "116\n",
      "117\n",
      "train done\n",
      "117\n",
      "118\n",
      "train done\n",
      "118\n",
      "119\n",
      "train done\n",
      "119\n",
      "120\n",
      "  Batch   120  of    256.\n",
      "train done\n",
      "120\n",
      "121\n",
      "train done\n",
      "121\n",
      "122\n",
      "train done\n",
      "122\n",
      "123\n",
      "train done\n",
      "123\n",
      "124\n",
      "train done\n",
      "124\n",
      "125\n",
      "train done\n",
      "125\n",
      "126\n",
      "train done\n",
      "126\n",
      "127\n",
      "train done\n",
      "127\n",
      "128\n",
      "train done\n",
      "128\n",
      "129\n",
      "train done\n",
      "129\n",
      "130\n",
      "  Batch   130  of    256.\n",
      "train done\n",
      "130\n",
      "131\n",
      "train done\n",
      "131\n",
      "132\n",
      "train done\n",
      "132\n",
      "133\n",
      "train done\n",
      "133\n",
      "134\n",
      "train done\n",
      "134\n",
      "135\n",
      "train done\n",
      "135\n",
      "136\n",
      "train done\n",
      "136\n",
      "137\n",
      "train done\n",
      "137\n",
      "138\n",
      "train done\n",
      "138\n",
      "139\n",
      "train done\n",
      "139\n",
      "140\n",
      "  Batch   140  of    256.\n",
      "train done\n",
      "140\n",
      "141\n",
      "train done\n",
      "141\n",
      "142\n",
      "train done\n",
      "142\n",
      "143\n",
      "train done\n",
      "143\n",
      "144\n",
      "train done\n",
      "144\n",
      "145\n",
      "train done\n",
      "145\n",
      "146\n",
      "train done\n",
      "146\n",
      "147\n",
      "train done\n",
      "147\n",
      "148\n",
      "train done\n",
      "148\n",
      "149\n",
      "train done\n",
      "149\n",
      "150\n",
      "  Batch   150  of    256.\n",
      "train done\n",
      "150\n",
      "151\n",
      "train done\n",
      "151\n",
      "152\n",
      "train done\n",
      "152\n",
      "153\n",
      "train done\n",
      "153\n",
      "154\n",
      "train done\n",
      "154\n",
      "155\n",
      "train done\n",
      "155\n",
      "156\n",
      "train done\n",
      "156\n",
      "157\n",
      "train done\n",
      "157\n",
      "158\n",
      "train done\n",
      "158\n",
      "159\n",
      "train done\n",
      "159\n",
      "160\n",
      "  Batch   160  of    256.\n",
      "train done\n",
      "160\n",
      "161\n",
      "train done\n",
      "161\n",
      "162\n",
      "train done\n",
      "162\n",
      "163\n",
      "train done\n",
      "163\n",
      "164\n",
      "train done\n",
      "164\n",
      "165\n",
      "train done\n",
      "165\n",
      "166\n",
      "train done\n",
      "166\n",
      "167\n",
      "train done\n",
      "167\n",
      "168\n",
      "train done\n",
      "168\n",
      "169\n",
      "train done\n",
      "169\n",
      "170\n",
      "  Batch   170  of    256.\n",
      "train done\n",
      "170\n",
      "171\n",
      "train done\n",
      "171\n",
      "172\n",
      "train done\n",
      "172\n",
      "173\n",
      "train done\n",
      "173\n",
      "174\n",
      "train done\n",
      "174\n",
      "175\n",
      "train done\n",
      "175\n",
      "176\n",
      "train done\n",
      "176\n",
      "177\n",
      "train done\n",
      "177\n",
      "178\n",
      "train done\n",
      "178\n",
      "179\n",
      "train done\n",
      "179\n",
      "180\n",
      "  Batch   180  of    256.\n",
      "train done\n",
      "180\n",
      "181\n",
      "train done\n",
      "181\n",
      "182\n",
      "train done\n",
      "182\n",
      "183\n",
      "train done\n",
      "183\n",
      "184\n",
      "train done\n",
      "184\n",
      "185\n",
      "train done\n",
      "185\n",
      "186\n",
      "train done\n",
      "186\n",
      "187\n",
      "train done\n",
      "187\n",
      "188\n",
      "train done\n",
      "188\n",
      "189\n",
      "train done\n",
      "189\n",
      "190\n",
      "  Batch   190  of    256.\n",
      "train done\n",
      "190\n",
      "191\n",
      "train done\n",
      "191\n",
      "192\n",
      "train done\n",
      "192\n",
      "193\n",
      "train done\n",
      "193\n",
      "194\n",
      "train done\n",
      "194\n",
      "195\n",
      "train done\n",
      "195\n",
      "196\n",
      "train done\n",
      "196\n",
      "197\n",
      "train done\n",
      "197\n",
      "198\n",
      "train done\n",
      "198\n",
      "199\n",
      "train done\n",
      "199\n",
      "200\n",
      "  Batch   200  of    256.\n",
      "train done\n",
      "200\n",
      "201\n",
      "train done\n",
      "201\n",
      "202\n",
      "train done\n",
      "202\n",
      "203\n",
      "train done\n",
      "203\n",
      "204\n",
      "train done\n",
      "204\n",
      "205\n",
      "train done\n",
      "205\n",
      "206\n",
      "train done\n",
      "206\n",
      "207\n",
      "train done\n",
      "207\n",
      "208\n",
      "train done\n",
      "208\n",
      "209\n",
      "train done\n",
      "209\n",
      "210\n",
      "  Batch   210  of    256.\n",
      "train done\n",
      "210\n",
      "211\n",
      "train done\n",
      "211\n",
      "212\n",
      "train done\n",
      "212\n",
      "213\n",
      "train done\n",
      "213\n",
      "214\n",
      "train done\n",
      "214\n",
      "215\n",
      "train done\n",
      "215\n",
      "216\n",
      "train done\n",
      "216\n",
      "217\n",
      "train done\n",
      "217\n",
      "218\n",
      "train done\n",
      "218\n",
      "219\n",
      "train done\n",
      "219\n",
      "220\n",
      "  Batch   220  of    256.\n",
      "train done\n",
      "220\n",
      "221\n",
      "train done\n",
      "221\n",
      "222\n",
      "train done\n",
      "222\n",
      "223\n",
      "train done\n",
      "223\n",
      "224\n",
      "train done\n",
      "224\n",
      "225\n",
      "train done\n",
      "225\n",
      "226\n",
      "train done\n",
      "226\n",
      "227\n",
      "train done\n",
      "227\n",
      "228\n",
      "train done\n",
      "228\n",
      "229\n",
      "train done\n",
      "229\n",
      "230\n",
      "  Batch   230  of    256.\n",
      "train done\n",
      "230\n",
      "231\n",
      "train done\n",
      "231\n",
      "232\n",
      "train done\n",
      "232\n",
      "233\n",
      "train done\n",
      "233\n",
      "234\n",
      "train done\n",
      "234\n",
      "235\n",
      "train done\n",
      "235\n",
      "236\n",
      "train done\n",
      "236\n",
      "237\n",
      "train done\n",
      "237\n",
      "238\n",
      "train done\n",
      "238\n",
      "239\n",
      "train done\n",
      "239\n",
      "240\n",
      "  Batch   240  of    256.\n",
      "train done\n",
      "240\n",
      "241\n",
      "train done\n",
      "241\n",
      "242\n",
      "train done\n",
      "242\n",
      "243\n",
      "train done\n",
      "243\n",
      "244\n",
      "train done\n",
      "244\n",
      "245\n",
      "train done\n",
      "245\n",
      "246\n",
      "train done\n",
      "246\n",
      "247\n",
      "train done\n",
      "247\n",
      "248\n",
      "train done\n",
      "248\n",
      "249\n",
      "train done\n",
      "249\n",
      "250\n",
      "  Batch   250  of    256.\n",
      "train done\n",
      "250\n",
      "251\n",
      "train done\n",
      "251\n",
      "252\n",
      "train done\n",
      "252\n",
      "253\n",
      "train done\n",
      "253\n",
      "254\n",
      "train done\n",
      "254\n",
      "255\n",
      "train done\n",
      "255\n",
      "\n",
      "Evaluating...\n",
      "0\n",
      "eval done\n",
      "1\n",
      "eval done\n",
      "2\n",
      "eval done\n",
      "3\n",
      "eval done\n",
      "\n",
      "Training Loss: 1.386\n",
      "Validation Loss: 1.401\n",
      "\n",
      " Epoch 2 / 2\n",
      "0\n",
      "train done\n",
      "0\n",
      "1\n",
      "train done\n",
      "1\n",
      "2\n",
      "train done\n",
      "2\n",
      "3\n",
      "train done\n",
      "3\n",
      "4\n",
      "train done\n",
      "4\n",
      "5\n",
      "train done\n",
      "5\n",
      "6\n",
      "train done\n",
      "6\n",
      "7\n",
      "train done\n",
      "7\n",
      "8\n",
      "train done\n",
      "8\n",
      "9\n",
      "train done\n",
      "9\n",
      "10\n",
      "  Batch    10  of    256.\n",
      "train done\n",
      "10\n",
      "11\n",
      "train done\n",
      "11\n",
      "12\n",
      "train done\n",
      "12\n",
      "13\n",
      "train done\n",
      "13\n",
      "14\n",
      "train done\n",
      "14\n",
      "15\n",
      "train done\n",
      "15\n",
      "16\n",
      "train done\n",
      "16\n",
      "17\n",
      "train done\n",
      "17\n",
      "18\n",
      "train done\n",
      "18\n",
      "19\n",
      "train done\n",
      "19\n",
      "20\n",
      "  Batch    20  of    256.\n",
      "train done\n",
      "20\n",
      "21\n",
      "train done\n",
      "21\n",
      "22\n",
      "train done\n",
      "22\n",
      "23\n",
      "train done\n",
      "23\n",
      "24\n",
      "train done\n",
      "24\n",
      "25\n",
      "train done\n",
      "25\n",
      "26\n",
      "train done\n",
      "26\n",
      "27\n",
      "train done\n",
      "27\n",
      "28\n",
      "train done\n",
      "28\n",
      "29\n",
      "train done\n",
      "29\n",
      "30\n",
      "  Batch    30  of    256.\n",
      "train done\n",
      "30\n",
      "31\n",
      "train done\n",
      "31\n",
      "32\n",
      "train done\n",
      "32\n",
      "33\n",
      "train done\n",
      "33\n",
      "34\n",
      "train done\n",
      "34\n",
      "35\n",
      "train done\n",
      "35\n",
      "36\n",
      "train done\n",
      "36\n",
      "37\n",
      "train done\n",
      "37\n",
      "38\n",
      "train done\n",
      "38\n",
      "39\n",
      "train done\n",
      "39\n",
      "40\n",
      "  Batch    40  of    256.\n",
      "train done\n",
      "40\n",
      "41\n",
      "train done\n",
      "41\n",
      "42\n",
      "train done\n",
      "42\n",
      "43\n",
      "train done\n",
      "43\n",
      "44\n",
      "train done\n",
      "44\n",
      "45\n",
      "train done\n",
      "45\n",
      "46\n",
      "train done\n",
      "46\n",
      "47\n",
      "train done\n",
      "47\n",
      "48\n",
      "train done\n",
      "48\n",
      "49\n",
      "train done\n",
      "49\n",
      "50\n",
      "  Batch    50  of    256.\n",
      "train done\n",
      "50\n",
      "51\n",
      "train done\n",
      "51\n",
      "52\n",
      "train done\n",
      "52\n",
      "53\n",
      "train done\n",
      "53\n",
      "54\n",
      "train done\n",
      "54\n",
      "55\n",
      "train done\n",
      "55\n",
      "56\n",
      "train done\n",
      "56\n",
      "57\n",
      "train done\n",
      "57\n",
      "58\n",
      "train done\n",
      "58\n",
      "59\n",
      "train done\n",
      "59\n",
      "60\n",
      "  Batch    60  of    256.\n",
      "train done\n",
      "60\n",
      "61\n",
      "train done\n",
      "61\n",
      "62\n",
      "train done\n",
      "62\n",
      "63\n",
      "train done\n",
      "63\n",
      "64\n",
      "train done\n",
      "64\n",
      "65\n",
      "train done\n",
      "65\n",
      "66\n",
      "train done\n",
      "66\n",
      "67\n",
      "train done\n",
      "67\n",
      "68\n",
      "train done\n",
      "68\n",
      "69\n",
      "train done\n",
      "69\n",
      "70\n",
      "  Batch    70  of    256.\n",
      "train done\n",
      "70\n",
      "71\n",
      "train done\n",
      "71\n",
      "72\n",
      "train done\n",
      "72\n",
      "73\n",
      "train done\n",
      "73\n",
      "74\n",
      "train done\n",
      "74\n",
      "75\n",
      "train done\n",
      "75\n",
      "76\n",
      "train done\n",
      "76\n",
      "77\n",
      "train done\n",
      "77\n",
      "78\n",
      "train done\n",
      "78\n",
      "79\n",
      "train done\n",
      "79\n",
      "80\n",
      "  Batch    80  of    256.\n",
      "train done\n",
      "80\n",
      "81\n",
      "train done\n",
      "81\n",
      "82\n",
      "train done\n",
      "82\n",
      "83\n",
      "train done\n",
      "83\n",
      "84\n",
      "train done\n",
      "84\n",
      "85\n",
      "train done\n",
      "85\n",
      "86\n",
      "train done\n",
      "86\n",
      "87\n",
      "train done\n",
      "87\n",
      "88\n",
      "train done\n",
      "88\n",
      "89\n",
      "train done\n",
      "89\n",
      "90\n",
      "  Batch    90  of    256.\n",
      "train done\n",
      "90\n",
      "91\n",
      "train done\n",
      "91\n",
      "92\n",
      "train done\n",
      "92\n",
      "93\n",
      "train done\n",
      "93\n",
      "94\n",
      "train done\n",
      "94\n",
      "95\n",
      "train done\n",
      "95\n",
      "96\n",
      "train done\n",
      "96\n",
      "97\n",
      "train done\n",
      "97\n",
      "98\n",
      "train done\n",
      "98\n",
      "99\n",
      "train done\n",
      "99\n",
      "100\n",
      "  Batch   100  of    256.\n",
      "train done\n",
      "100\n",
      "101\n",
      "train done\n",
      "101\n",
      "102\n",
      "train done\n",
      "102\n",
      "103\n",
      "train done\n",
      "103\n",
      "104\n",
      "train done\n",
      "104\n",
      "105\n",
      "train done\n",
      "105\n",
      "106\n",
      "train done\n",
      "106\n",
      "107\n",
      "train done\n",
      "107\n",
      "108\n",
      "train done\n",
      "108\n",
      "109\n",
      "train done\n",
      "109\n",
      "110\n",
      "  Batch   110  of    256.\n",
      "train done\n",
      "110\n",
      "111\n",
      "train done\n",
      "111\n",
      "112\n",
      "train done\n",
      "112\n",
      "113\n",
      "train done\n",
      "113\n",
      "114\n",
      "train done\n",
      "114\n",
      "115\n",
      "train done\n",
      "115\n",
      "116\n",
      "train done\n",
      "116\n",
      "117\n",
      "train done\n",
      "117\n",
      "118\n",
      "train done\n",
      "118\n",
      "119\n",
      "train done\n",
      "119\n",
      "120\n",
      "  Batch   120  of    256.\n",
      "train done\n",
      "120\n",
      "121\n",
      "train done\n",
      "121\n",
      "122\n",
      "train done\n",
      "122\n",
      "123\n",
      "train done\n",
      "123\n",
      "124\n",
      "train done\n",
      "124\n",
      "125\n",
      "train done\n",
      "125\n",
      "126\n",
      "train done\n",
      "126\n",
      "127\n",
      "train done\n",
      "127\n",
      "128\n",
      "train done\n",
      "128\n",
      "129\n",
      "train done\n",
      "129\n",
      "130\n",
      "  Batch   130  of    256.\n",
      "train done\n",
      "130\n",
      "131\n",
      "train done\n",
      "131\n",
      "132\n",
      "train done\n",
      "132\n",
      "133\n",
      "train done\n",
      "133\n",
      "134\n",
      "train done\n",
      "134\n",
      "135\n",
      "train done\n",
      "135\n",
      "136\n",
      "train done\n",
      "136\n",
      "137\n",
      "train done\n",
      "137\n",
      "138\n",
      "train done\n",
      "138\n",
      "139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n",
      "139\n",
      "140\n",
      "  Batch   140  of    256.\n",
      "train done\n",
      "140\n",
      "141\n",
      "train done\n",
      "141\n",
      "142\n",
      "train done\n",
      "142\n",
      "143\n",
      "train done\n",
      "143\n",
      "144\n",
      "train done\n",
      "144\n",
      "145\n",
      "train done\n",
      "145\n",
      "146\n",
      "train done\n",
      "146\n",
      "147\n",
      "train done\n",
      "147\n",
      "148\n",
      "train done\n",
      "148\n",
      "149\n",
      "train done\n",
      "149\n",
      "150\n",
      "  Batch   150  of    256.\n",
      "train done\n",
      "150\n",
      "151\n",
      "train done\n",
      "151\n",
      "152\n",
      "train done\n",
      "152\n",
      "153\n",
      "train done\n",
      "153\n",
      "154\n",
      "train done\n",
      "154\n",
      "155\n",
      "train done\n",
      "155\n",
      "156\n",
      "train done\n",
      "156\n",
      "157\n",
      "train done\n",
      "157\n",
      "158\n",
      "train done\n",
      "158\n",
      "159\n",
      "train done\n",
      "159\n",
      "160\n",
      "  Batch   160  of    256.\n",
      "train done\n",
      "160\n",
      "161\n",
      "train done\n",
      "161\n",
      "162\n",
      "train done\n",
      "162\n",
      "163\n",
      "train done\n",
      "163\n",
      "164\n",
      "train done\n",
      "164\n",
      "165\n",
      "train done\n",
      "165\n",
      "166\n",
      "train done\n",
      "166\n",
      "167\n",
      "train done\n",
      "167\n",
      "168\n",
      "train done\n",
      "168\n",
      "169\n",
      "train done\n",
      "169\n",
      "170\n",
      "  Batch   170  of    256.\n",
      "train done\n",
      "170\n",
      "171\n",
      "train done\n",
      "171\n",
      "172\n",
      "train done\n",
      "172\n",
      "173\n",
      "train done\n",
      "173\n",
      "174\n",
      "train done\n",
      "174\n",
      "175\n",
      "train done\n",
      "175\n",
      "176\n",
      "train done\n",
      "176\n",
      "177\n",
      "train done\n",
      "177\n",
      "178\n",
      "train done\n",
      "178\n",
      "179\n",
      "train done\n",
      "179\n",
      "180\n",
      "  Batch   180  of    256.\n",
      "train done\n",
      "180\n",
      "181\n",
      "train done\n",
      "181\n",
      "182\n",
      "train done\n",
      "182\n",
      "183\n",
      "train done\n",
      "183\n",
      "184\n",
      "train done\n",
      "184\n",
      "185\n",
      "train done\n",
      "185\n",
      "186\n",
      "train done\n",
      "186\n",
      "187\n",
      "train done\n",
      "187\n",
      "188\n",
      "train done\n",
      "188\n",
      "189\n",
      "train done\n",
      "189\n",
      "190\n",
      "  Batch   190  of    256.\n",
      "train done\n",
      "190\n",
      "191\n",
      "train done\n",
      "191\n",
      "192\n",
      "train done\n",
      "192\n",
      "193\n",
      "train done\n",
      "193\n",
      "194\n",
      "train done\n",
      "194\n",
      "195\n",
      "train done\n",
      "195\n",
      "196\n",
      "train done\n",
      "196\n",
      "197\n",
      "train done\n",
      "197\n",
      "198\n",
      "train done\n",
      "198\n",
      "199\n",
      "train done\n",
      "199\n",
      "200\n",
      "  Batch   200  of    256.\n",
      "train done\n",
      "200\n",
      "201\n",
      "train done\n",
      "201\n",
      "202\n",
      "train done\n",
      "202\n",
      "203\n",
      "train done\n",
      "203\n",
      "204\n",
      "train done\n",
      "204\n",
      "205\n",
      "train done\n",
      "205\n",
      "206\n",
      "train done\n",
      "206\n",
      "207\n",
      "train done\n",
      "207\n",
      "208\n",
      "train done\n",
      "208\n",
      "209\n",
      "train done\n",
      "209\n",
      "210\n",
      "  Batch   210  of    256.\n",
      "train done\n",
      "210\n",
      "211\n",
      "train done\n",
      "211\n",
      "212\n",
      "train done\n",
      "212\n",
      "213\n",
      "train done\n",
      "213\n",
      "214\n",
      "train done\n",
      "214\n",
      "215\n",
      "train done\n",
      "215\n",
      "216\n",
      "train done\n",
      "216\n",
      "217\n",
      "train done\n",
      "217\n",
      "218\n",
      "train done\n",
      "218\n",
      "219\n",
      "train done\n",
      "219\n",
      "220\n",
      "  Batch   220  of    256.\n",
      "train done\n",
      "220\n",
      "221\n",
      "train done\n",
      "221\n",
      "222\n",
      "train done\n",
      "222\n",
      "223\n",
      "train done\n",
      "223\n",
      "224\n",
      "train done\n",
      "224\n",
      "225\n",
      "train done\n",
      "225\n",
      "226\n",
      "train done\n",
      "226\n",
      "227\n",
      "train done\n",
      "227\n",
      "228\n",
      "train done\n",
      "228\n",
      "229\n",
      "train done\n",
      "229\n",
      "230\n",
      "  Batch   230  of    256.\n",
      "train done\n",
      "230\n",
      "231\n",
      "train done\n",
      "231\n",
      "232\n",
      "train done\n",
      "232\n",
      "233\n",
      "train done\n",
      "233\n",
      "234\n",
      "train done\n",
      "234\n",
      "235\n",
      "train done\n",
      "235\n",
      "236\n",
      "train done\n",
      "236\n",
      "237\n",
      "train done\n",
      "237\n",
      "238\n",
      "train done\n",
      "238\n",
      "239\n",
      "train done\n",
      "239\n",
      "240\n",
      "  Batch   240  of    256.\n",
      "train done\n",
      "240\n",
      "241\n",
      "train done\n",
      "241\n",
      "242\n",
      "train done\n",
      "242\n",
      "243\n",
      "train done\n",
      "243\n",
      "244\n",
      "train done\n",
      "244\n",
      "245\n",
      "train done\n",
      "245\n",
      "246\n",
      "train done\n",
      "246\n",
      "247\n",
      "train done\n",
      "247\n",
      "248\n",
      "train done\n",
      "248\n",
      "249\n",
      "train done\n",
      "249\n",
      "250\n",
      "  Batch   250  of    256.\n",
      "train done\n",
      "250\n",
      "251\n",
      "train done\n",
      "251\n",
      "252\n",
      "train done\n",
      "252\n",
      "253\n",
      "train done\n",
      "253\n",
      "254\n",
      "train done\n",
      "254\n",
      "255\n",
      "train done\n",
      "255\n",
      "\n",
      "Evaluating...\n",
      "0\n",
      "eval done\n",
      "1\n",
      "eval done\n",
      "2\n",
      "eval done\n",
      "3\n",
      "eval done\n",
      "\n",
      "Training Loss: 1.378\n",
      "Validation Loss: 1.388\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate(dev_dataloader)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(transfer_model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "afcd2e08",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RmML2x8Pb20d",
    "outputId": "7a7b0aac-4ba4-4089-fdcd-b3a968c2ca1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "transfer_model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "30f09d2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAYp5a2wBH_e",
    "outputId": "43ff034d-f8d0-455c-807e-3702ea8a7bae",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating...\n",
      "0\n",
      "eval done\n",
      "1\n",
      "eval done\n",
      "2\n",
      "eval done\n",
      "3\n",
      "eval done\n",
      "4\n",
      "eval done\n",
      "5\n",
      "eval done\n",
      "6\n",
      "eval done\n",
      "7\n",
      "eval done\n",
      "8\n",
      "eval done\n",
      "9\n",
      "eval done\n",
      "10\n",
      "  Batch    10  of     39.\n",
      "eval done\n",
      "11\n",
      "eval done\n",
      "12\n",
      "eval done\n",
      "13\n",
      "eval done\n",
      "14\n",
      "eval done\n",
      "15\n",
      "eval done\n",
      "16\n",
      "eval done\n",
      "17\n",
      "eval done\n",
      "18\n",
      "eval done\n",
      "19\n",
      "eval done\n",
      "20\n",
      "  Batch    20  of     39.\n",
      "eval done\n",
      "21\n",
      "eval done\n",
      "22\n",
      "eval done\n",
      "23\n",
      "eval done\n",
      "24\n",
      "eval done\n",
      "25\n",
      "eval done\n",
      "26\n",
      "eval done\n",
      "27\n",
      "eval done\n",
      "28\n",
      "eval done\n",
      "29\n",
      "eval done\n",
      "30\n",
      "  Batch    30  of     39.\n",
      "eval done\n",
      "31\n",
      "eval done\n",
      "32\n",
      "eval done\n",
      "33\n",
      "eval done\n",
      "34\n",
      "eval done\n",
      "35\n",
      "eval done\n",
      "36\n",
      "eval done\n",
      "37\n",
      "eval done\n",
      "38\n",
      "eval done\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on batches of test data, collect final predictions\n",
    "_, preds = evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c513c0a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fD-wc3zcZBc",
    "outputId": "05cc6c4a-42a5-429d-b059-5f4b0bb27e71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       388\n",
      "           1       0.18      0.51      0.27       201\n",
      "           2       0.54      0.56      0.55       599\n",
      "           3       0.02      0.02      0.02        45\n",
      "\n",
      "    accuracy                           0.35      1233\n",
      "   macro avg       0.19      0.27      0.21      1233\n",
      "weighted avg       0.29      0.35      0.31      1233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted classes\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "\n",
    "# Print model performance for each class\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4243456",
   "metadata": {},
   "source": [
    "Future steps to improve model performance:\n",
    "\n",
    " - Longer sequences of input text\n",
    " - Trying other different BERT models\n",
    " - Generating data using GANs for under-represented classes instead of introducing class weights\n",
    " - Partial tuning of BERT weights instead of completely fixing them\n",
    " - More number of epochs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Fact Checking Classifier using HuggingFace.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
